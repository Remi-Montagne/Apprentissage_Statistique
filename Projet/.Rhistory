tree_predit=predict(model_prune, newdata=test, type="class")
error_tree <- mean(tree_predit != test$Y)
plot(model_prune)
text(model_prune, cex=0.7)
set.seed(123)
CV_tree = cv.tree(model_tree, FUN = prune.misclass) #remarque : par defaut, on a FUN=prune.tree pas bon ça correspond a l'erreur d'apprentissage
CV_tree
plot(CV_tree$size, CV_tree$dev, type = 'b')
### Obtention du modele
model_tree <- tree(Y~., data = train)
summary(model_tree)
plot(model_tree)
text(model_tree, cex=0.7)
### Raffinement du modele : elagage
# etude d'elagages possibles
set.seed(123)
CV_tree = cv.tree(model_tree, FUN = prune.misclass) #remarque : par defaut, on a FUN=prune.tree pas bon ça correspond a l'erreur d'apprentissage
CV_tree
plot(CV_tree$size, CV_tree$dev, type = 'b')
# selection du plus petit nombre de feuilles donnant la plus petite erreur de prediction
nb_feuille_candidat <- which(CV_tree$dev == min(CV_tree$dev))
feuilles <- CV_tree$size[nb_feuille_candidat]
feuilles_min <- min(feuilles)
# nouvel arbre
model_prune <- prune.misclass(model_tree, best=feuilles_min)
plot(model_prune)
text(model_prune, cex=0.7)
### Evaluation du model sur le jeu test
tree_predit=predict(model_prune, newdata=test, type="class")
error_tree <- mean(tree_predit != test$Y)
error_tree
#################################
#      Regression Ridge         #
#################################
### Obtention d'un premier modele
xtrain=model.matrix(Y~., train)[,-1] #passage de la dataframe en matrice
ytrain = train$Y
xtest=model.matrix(Y~., test)[,-1]
model_ridge <- glmnet(xtrain,ytrain, family = "binomial", alpha = 0)
### Choix du lambda par validation croisee
# 1e test
set.seed(123)
cv.out= cv.glmnet(xtrain, ytrain, family = "binomial", alpha = 0)
plot(cv.out) #trace cvm en fonction de lambda
# minimisation du lambda
lambda_seq = seq(0.0001, 0.01, 0.0001)
set.seed(123)
cv.out= cv.glmnet(xtrain, ytrain, family = "binomial", alpha = 0, lambda = lambda_seq)
plot(cv.out) #trace cvm en fonction de lambda
lambda_opt = cv.out$lambda.min
lambda_opt
### Prédiction d'un modele avec le lambda optimal
model_ridge = glmnet(xtrain, ytrain, family = "binomial", alpha = 0, lambda = lambda_seq)
### Evaluation du model sur le jeu test
ridge_predit <- predict(object = model_ridge, newx = xtest, s = lambda_opt, type = "class")
table(test$Y, ridge_predit)
error_ridge <- mean(test$Y != ridge_predit)
#################################
#        Random Forest          #
#################################
### Construction d'une foret aleatoire
set.seed(123)
model_rf <- randomForest(Y~., data = train)
print(model_rf) #affiche un summary de la foret
### Erreur test
rf_predit=predict(model_rf, newdata=test, type="class")
table(test$Y, rf_predit)
error_rf <- mean(test$Y != rf_predit)
#################################
#   Comparaison des erreur et   #
#  construction du model final  #
#################################
### Taux de mal classes
cat("arbres CART :", error_tree, "\n","ridge :", error_ridge, "\n","random forest :", error_rf, "\n\n\n")
### Taux de mal classes
cat("arbre CART :", error_tree, "\n","ridge :", error_ridge, "\n","random forest :", error_rf, "\n\n\n")
Se_tree <- mc_tree[2,2]/(mc_tree[2,1] + mc_tree[2,2])
#arbre CART
mc_tree <- table(test$Y, tree_predit)
mc_tree
Se_tree <- mc_tree[2,2]/(mc_tree[2,1] + mc_tree[2,2])
Se_tree
Se_ridge <- mc_ridge[2,2]/(mc_ridge[2,1] + mc_ridge[2,2])
Se_ridge
#methode ridge
mc_ridge <- table(test$Y, ridge_predit)
mc_ridge
Se_ridge <- mc_ridge[2,2]/(mc_ridge[2,1] + mc_ridge[2,2])
Se_ridge
#arbre CART
mc_tree <- table(test$Y, tree_predit)
mc_tree
Se_tree <- mc_tree[2,2]/(mc_tree[2,1] + mc_tree[2,2])
Se_tree
#methode ridge
mc_ridge <- table(test$Y, ridge_predit)
mc_ridge
Se_ridge <- mc_ridge[2,2]/(mc_ridge[2,1] + mc_ridge[2,2])
Se_ridge
#methode rf
mc_rf <- table(test$Y, rf_predit)
mc_rf
Se_rf <- mc_rf[2,2]/(mc_rf[2,1] + mc_rf[2,2])
Se_rf
### Coursbes ROC
#pour l'arbre CART
pred_tree = predict(model_tree, newdata = test)
predic_tree = prediction(pred_tree[,2], test$Y)
perf_tree <- performance(predic_tree, "tpr", "fpr")
plot(perf_tree, main = "Courbes ROC")
#pour la methode ridge
pred_ridge = predict(model_ridge, newx = xtest, s = lambda_opt, type = "response")
predic_ridge = prediction(pred_ridge[,1], test$Y)
perf_ridge <- performance(predic_ridge, "tpr", "fpr")
plot(perf_ridge, col = 2, add = TRUE)
#pour la random forest
pred_rf = predict(model_rf, newdata = test, type = "prob")
predic_rf = prediction(pred_rf[,2], test$Y)
perf_rf <- performance(predic_rf, "tpr", "fpr")
plot(perf_rf, col = 3, add = TRUE)
abline(a = 0, b = 1, col = 4)
model <- randomForest(Y~., data = tab2)
pred=predict(model_rf, newdata=tab2, type="class")
table(tab2$Y, pred)
error <- mean(tab2$Y != pred)
error
#################################
#           Arbre CART          #
#################################
### Obtention du modele
model_tree <- tree(Y~., data = train)
summary(model_tree)
plot(model_tree)
text(model_tree, cex=0.7)
### Raffinement du modele : elagage
# etude d'elagages possibles
set.seed(123)
CV_tree = cv.tree(model_tree, FUN = prune.misclass) #remarque : par defaut, on a FUN=prune.tree pas bon ça correspond a l'erreur d'apprentissage
CV_tree
plot(CV_tree$size, CV_tree$dev, type = 'b')
# selection du plus petit nombre de feuilles donnant la plus petite erreur de prediction
nb_feuille_candidat <- which(CV_tree$dev == min(CV_tree$dev))
feuilles <- CV_tree$size[nb_feuille_candidat]
feuilles_min <- min(feuilles)
# nouvel arbre
model_prune <- prune.misclass(model_tree, best=feuilles_min)
plot(model_prune)
text(model_prune, cex=0.7)
### Erreur test de l'arbre
tree_predit=predict(model_prune, newdata=test, type="class")
error_tree <- mean(tree_predit != test$Y)
?is.factor tab2[1]
?is.factor
levels(tab2[1])
levels(tab2)
is.ordered(tab2[,1])
levels(tab2[,1])
tab2[tab2[,1]=="M",] <- 0
tab2[tab2[,1]=="B",] <- 1
View(tab2)
#################################
# Chargement data et librairies #
#################################
### chargement dee données
tab <- read.table(file = "wdbc.data", header = FALSE, sep = ',')
names(tab)[2]="Y" #la colonne 2 est la variable reponse, renommee Y
names(tab)[1]="Id" #la colonne 1 est l'identifiant, renommé Id
tab2 <- tab[,-1] # data2 reprend la même table mais sans la colonne des identifiants
#tab_mean <- tab[,2:12] #on selectionne uniquement les valeur moyennes des 10 criteres etudies
lin <- nrow(tab2) #nombre de lignes
lin
col <- ncol(tab2) #nombre de lignes
col
tab2[tab2[,1]=="M",] <- "0"
tab2[tab2[,1]=="B",] <- "1"
?factor
### chargement dee données
tab <- read.table(file = "wdbc.data", header = FALSE, sep = ',')
names(tab)[2]="Y" #la colonne 2 est la variable reponse, renommee Y
names(tab)[1]="Id" #la colonne 1 est l'identifiant, renommé Id
tab2 <- tab[,-1] # data2 reprend la même table mais sans la colonne des identifiants
#tab_mean <- tab[,2:12] #on selectionne uniquement les valeur moyennes des 10 criteres etudies
lin <- nrow(tab2) #nombre de lignes
lin
col <- ncol(tab2) #nombre de lignes
col
tab2[tab2[,1]=="M",] <- factor("0")
tab2[tab2[,1]=="B",] <- factor("1")
#pour l'arbre CART
pred_tree = predict(model_tree, newdata = test)
predic_tree = prediction(pred_tree[,2], test$Y)
perf_tree <- performance(predic_tree, "tpr", "fpr")
plot(perf_tree, main = "Courbes ROC")
#pour la methode ridge
pred_ridge = predict(model_ridge, newx = xtest, s = lambda_opt, type = "response")
predic_ridge = prediction(pred_ridge[,1], test$Y)
perf_ridge <- performance(predic_ridge, "tpr", "fpr")
plot(perf_ridge, col = 2, add = TRUE)
#pour la random forest
pred_rf = predict(model_rf, newdata = test, type = "prob")
predic_rf = prediction(pred_rf[,2], test$Y)
perf_rf <- performance(predic_rf, "tpr", "fpr")
plot(perf_rf, col = 3, add = TRUE)
abline(a = 0, b = 1, col = 4)
mc_tree <- table(test$Y, tree_predit)
mc_tree
?table
tr(mc_tree)
mean(tab2[,1] == "M")
mean(tab2[,1] == factor("M"))
### chargement dee données
tab <- read.table(file = "wdbc.data", header = FALSE, sep = ',')
names(tab)[2]="Y" #la colonne 2 est la variable reponse, renommee Y
names(tab)[1]="Id" #la colonne 1 est l'identifiant, renommé Id
tab2 <- tab[,-1] # data2 reprend la même table mais sans la colonne des identifiants
#tab_mean <- tab[,2:12] #on selectionne uniquement les valeur moyennes des 10 criteres etudies
lin <- nrow(tab2) #nombre de lignes
lin
col <- ncol(tab2) #nombre de lignes
coltab2[tab2[,1]=="M",] <- 0
### chargement dee données
tab <- read.table(file = "wdbc.data", header = FALSE, sep = ',')
names(tab)[2]="Y" #la colonne 2 est la variable reponse, renommee Y
names(tab)[1]="Id" #la colonne 1 est l'identifiant, renommé Id
tab2 <- tab[,-1] # data2 reprend la même table mais sans la colonne des identifiants
#tab_mean <- tab[,2:12] #on selectionne uniquement les valeur moyennes des 10 criteres etudies
lin <- nrow(tab2) #nombre de lignes
lin
col <- ncol(tab2) #nombre de lignes
mean(tab2[,1] == "M")
mean(tab2[,1] == "B")
mean(tab2[,1] == "B")
mean(train[,1] == "B")
mean(test[,1] == "B")
cat("proportion de B dans le tableau total : ", mean(tab2[,1] == "B"))
cat("proportion de B dans train : ", mean(train[,1] == "B"))
cat("proportion de B dans test : ", mean(test[,1] == "B"))
model_glm <- glm(Y~., data = train, family = binomial)
summary(model_glm)
model_step <- step(model_glm, direction = "both")
summary(model_step)
summary(model_glm)
model_glm <- glm(Y~., data = train, family = "binomial")
summary(model_glm)
model_step <- step(model_glm, direction = "both")
summary(model_step)
warning()
model_step <- step(model_glm, direction = "both")
warning()
warnings()
model_glm <- glm(Y~., data = train, family =binomial)
summary(model_glm)
?step
model_zero <- glm(Y~1, data = train, family =binomial)
model_step <- step(model_zero, model_glm, direction = "both")
summary(model_step)
model_zero <- glm(Y~1, data = train, family =binomial)
model_glm <- glm(Y~., data = train, family =binomial)
summary(model_glm)
model_step <- step(model_zero, scope = c(model_zero,model_glm), direction = "both")
summary(model_step)
model_zero <- glm(Y~V3, data = train, family =binomial)
model_glm <- glm(Y~., data = train, family =binomial)
summary(model_glm)
model_step <- step(model_zero, scope = c(model_zero,model_glm), direction = "both")
summary(model_step)
model_zero <- glm(Y~V3, data = train, family =binomial)
model_glm <- glm(Y~., data = train, family =binomial)
summary(model_glm)
model_step <- step(model_zero, scope = c(model_zero,model_glm), direction = "forward")
summary(model_step)
model_glm <- glm(Y~., data = train, family =binomial)
summary(model_glm)
model_step <- step(model_glm, direction = "backward")
summary(model_step)
#erreur apprentissage
glm_predit = rep('B', length(train$Y)) #on va mettre nos predictions (M ou B) dedans
probs = predict(model_glm, type = "response") #prediction de proba
glm_predit[probs > 0.5] = 'M' #si on est au-dessus de 0.5, on met M dans glm-predit. Sinon on laisse les B, qui y sont déjà.
mean(train$Y!=glm_predit) #moyenne des mal classes
#erreur apprentissage
glm_predit = rep("B", length(train$Y)) #on va mettre nos predictions (M ou B) dedans
probs = predict(model_glm, type = "response") #prediction de proba
probs
glm_predit[probs > 0.5] <- 'M'
glm_predit
mean(train$Y!=glm_predit)
#erreur apprentissage
glm_predit = rep("B", length(train$Y)) #on va mettre nos predictions (M ou B) dedans
probs = predict(model_step, type = "response") #prediction de proba
glm_predit[probs > 0.5] <- 'M' #si on est au-dessus de 0.5, on met M dans glm-predit. Sinon on laisse les B, qui y sont déjà.
mean(train$Y!=glm_predit) #moyenne des mal classes
summary(model_step)
?table
glm_predit = rep("B", length(test$Y)) #on va mettre nos predictions (M ou B) dedans
probs = predict(model_step, newdata = test, type = "response") #prediction de proba
glm_predit[probs > 0.5] <- 'M' #si on est au-dessus de 0.5, on met M dans glm-predit. Sinon on laisse les B, qui y sont déjà.
table(train$Y, glm_predit)
table(test$Y, glm_predit)
mean(train$Y!=glm_predit) #moyenne des mal classes
mean(test$Y!=glm_predit) #moyenne des mal classes
summary(model_step)
pairs(tab2[,1:11])
pairs(tab2[,11:21])
pairs(tab2[,1:11])
tab3 <- tab[,-c(5,6,10,15,16,20,25,26,30)]
model_total <- glm(Y~., data = train, family =binomial)
summary(model_glm)
model_total <- glm(Y~., data = train, family = "binomial")
summary(model_glm)
train_gml = tab3[-test]
test_gml = tab3[test]
tab3 <- tab[,-c(5,6,10,15,16,20,25,26,30)]
train_gml = tab3[-test]
test_gml = tab3[test]
train_gml = tab3[train,]
test_gml = tab3[,test,]
train_gml = tab3[train,]
train_gml = tab3[train, ]
View(tab3)
tab3 <- tab[,-c(1,5,6,10,15,16,20,25,26,30)]
tab3 <- tab[,-c(1,5,6,10,15,16,20,25,26,30)]
train_gml = tab3[train,]
tab3 <- tab[,-c(1,5,6,10,15,16,20,25,26,30)]
set.seed(123) #afin d'obtenir des resultats reproductibles, la graine est fixee, ici a 123
test = sample(1:lin, round(lin/3))
train = -test
train_gml = tab3[train,]
test_gml = tab3[test,]
model_total <- glm(Y~., data = train_gml, family = "binomial")
model_total <- glm(Y~., data = train_gml, family = binomial)
summary(model)
summary(model_total)
View(tab3)
model_total <- glm(Y~., data = train_gml[,c(1:6,14:22)], family = "binomial")
summary(model_total)
model_gml <- glm(Y~., data = train_gml[,c(1:6,14:22)], family = "binomial")
summary(model_total)
model_step = step(model_glm, direction = "backward")
summary(model_step)
model_min <- glm(Y~1, data = train, family = "binomial")
model_total <- glm(Y~., data = train, family = "binomial")
summary(model_total)
# n'a pas correlé
step(model_min,scope=list(upper=model_total),trace=1,direction="forward")
model_min <- glm(Y~1, data = train, family = "binomial")
model_total <- glm(Y~., data = train, family = "binomial")
summary(model_total)
# n'a pas convergé
step(model_min,scope=list(upper=model_total),trace=1,direction="forward")
step(model_min,scope=list(upper=model_total),direction="forward")
model_min <- glm(Y~1, data = train, family = "binomial")
model_min <- glm(Y~1, data = train, family = binomial)
set.seed(123) #afin d'obtenir des resultats reproductibles, la graine est fixee, ici a 123
test = sample(1:lin, round(lin/3))
train = -test
train = tab2[train,]
test = tab2[test,]
model_min <- glm(Y~1, data = train, family = "binomial")
model_total <- glm(Y~., data = train, family = "binomial")
step(model_min,scope=list(upper=model_total),direction="forward")
model_step <- step(model_min,scope=list(upper=model_total),direction="forward")
summary(model_step)
tab3 <- tab[,-c(1,5,6,10,15,16,20,25,26,30)]
set.seed(123) #afin d'obtenir des resultats reproductibles, la graine est fixee, ici a 123
test = sample(1:lin, round(lin/3))
train = -test
train_gml = tab3[train,]
test_gml = tab3[test,]
model_step <- step(model_min,scope=list(upper=model_total),direction="both")
summary(model_step)
#modele non significatif
model_step <- step(model_total,scope=list(lower=model_min),direction="both")
summary(model_step)
#modele non significatif
model_step <- step(model_total,scope=list(lower=model_min),direction="backward")
summary(model_step)
model_min <- glm(Y~1, data = train[,1:11], family = "binomial")
model_total <- glm(Y~., data = train[,1:11], family = "binomial")
summary(model_total)
rm(list=ls())
#################################
# Chargement data et librairies #
#################################
### chargement dee données
tab <- read.table(file = "wdbc.data", header = FALSE, sep = ',')
names(tab)[2]="Y" #la colonne 2 est la variable reponse, renommee Y
names(tab)[1]="Id" #la colonne 1 est l'identifiant, renommé Id
tab2 <- tab[,-1] # data2 reprend la même table mais sans la colonne des identifiants
#tab_mean <- tab[,2:12] #on selectionne uniquement les valeur moyennes des 10 criteres etudies
lin <- nrow(tab2) #nombre de lignes
lin
col <- ncol(tab2) #nombre de lignes
### chargement des librairies
library(tree)
library(glmnet)
library(randomForest)
library(ROCR)
#################################
#    Observation des données    #
#################################
### observation de correlation entre variables
pairs(tab2[,1:11])
round(cor(tab2[,2:col]), digits = 2)
### boxplots : distribution des variable en fonction de la variable réponse
for(i in 2:col) {
boxplot(tab2[,i]~tab2$Y,xlab = toString(i+1))
}
#################################
#      Generation des jeux      #
# d'entrainement et de test     #
#################################
set.seed(123) #afin d'obtenir des resultats reproductibles, la graine est fixee, ici a 123
test = sample(1:lin, round(lin/3))
train = -test
train = tab2[train,]
test = tab2[test,]
#verification que les proportions de chaque type de patient sont similare dans les differents jeux de donnees
cat("proportion de B dans le tableau total : ", mean(tab2[,1] == "B"))
cat("proportion de B dans train : ", mean(train[,1] == "B"))
cat("proportion de B dans test : ", mean(test[,1] == "B"))
#################################
#     Regression logistique     #
#################################
model_min <- glm(Y~1, data = train[,1:11], family = "binomial")
model_total <- glm(Y~., data = train[,1:11], family = "binomial")
summary(model_total)
# n'a pas convergé
model_step <- step(model_total,scope=list(lower=model_min),direction="backward")
summary(model_step)
#modele non significatif
############################################
#     Projet Apprentissage Statistique     #
############################################
rm(list=ls())
#################################
# Chargement data et librairies #
#################################
### chargement dee données
tab <- read.table(file = "wdbc.data", header = FALSE, sep = ',')
names(tab)[2]="Y" #la colonne 2 est la variable reponse, renommee Y
names(tab)[1]="Id" #la colonne 1 est l'identifiant, renommé Id
tab2 <- tab[,-1] # data2 reprend la même table mais sans la colonne des identifiants
#tab_mean <- tab[,2:12] #on selectionne uniquement les valeur moyennes des 10 criteres etudies
lin <- nrow(tab2) #nombre de lignes
lin
col <- ncol(tab2) #nombre de lignes
### chargement des librairies
library(tree)
library(glmnet)
library(randomForest)
library(ROCR)
#################################
#    Observation des données    #
#################################
### observation de correlation entre variables
pairs(tab2[,1:11])
round(cor(tab2[,2:col]), digits = 2)
### boxplots : distribution des variable en fonction de la variable réponse
for(i in 2:col) {
boxplot(tab2[,i]~tab2$Y,xlab = toString(i+1))
}
#################################
#      Generation des jeux      #
# d'entrainement et de test     #
#################################
set.seed(123) #afin d'obtenir des resultats reproductibles, la graine est fixee, ici a 123
test = sample(1:lin, round(lin/3))
train = -test
train = tab2[train,]
test = tab2[test,]
#verification que les proportions de chaque type de patient sont similare dans les differents jeux de donnees
cat("proportion de B dans le tableau total : ", mean(tab2[,1] == "B"))
cat("proportion de B dans train : ", mean(train[,1] == "B"))
cat("proportion de B dans test : ", mean(test[,1] == "B"))
#################################
#     Regression logistique     #
#################################
model_min <- glm(Y~1, data = train[,c(1:11,21:col)], family = "binomial")
model_total <- glm(Y~., data = train[,c(1:11,21:col)], family = "binomial")
)
summary(model_total)
model_min <- glm(Y~1, data = train[,c(1:11)], family = "binomial")
model_total <- glm(Y~., data = train[,c(1:11)], family = "binomial")
summary(model_total)
round(cor(tab2[,2:col]), digits = 2)
model_total2 <- glm(Y~., data = train[,1:11], family = "binomial")
summary(model_total)
model_step = step(model_glm, direction = "both")
summary(model_step)
model_step = step(model_total, direction = "both")
summary(model_step)
model_glm = step(model_total, direction = "both")
summary(model_step)
glm_predit = rep("B", length(test$Y)) #on va mettre nos predictions (M ou B) dedans
glm_predit = rep("B", length(test$Y)) #on va mettre nos predictions (M ou B) dedans
probs = predict(model_glm, newdata = test, type = "response") #prediction de proba
glm_predit[probs > 0.5] <- 'M' #si on est au-dessus de 0.5, on met M dans glm-predit. Sinon on laisse les B, qui y sont déjà.
table(test$Y, glm_predit)
mean(test$Y!=glm_predit) #moyenne des mal classes
table(train$Y, glm_predit)
mean(train$Y!=glm_predit) #moyenne des mal classes
glm_predit = rep("B", length(test$Y)) #on va mettre nos predictions (M ou B) dedans
probs = predict(model_glm, newdata = train, type = "response") #prediction de proba
glm_predit[probs > 0.5] <- 'M' #si on est au-dessus de 0.5, on met M dans glm-predit. Sinon on laisse les B, qui y sont déjà.
table(train$Y, glm_predit)
mean(train$Y!=glm_predit) #moyenne des mal classes
glm_predit = rep("B", length(train$Y)) #on va mettre nos predictions (M ou B) dedans
probs = predict(model_glm, newdata = train, type = "response") #prediction de proba
glm_predit[probs > 0.5] <- 'M' #si on est au-dessus de 0.5, on met M dans glm-predit. Sinon on laisse les B, qui y sont déjà.
table(train$Y, glm_predit)
mean(train$Y!=glm_predit) #moyenne des mal classes
